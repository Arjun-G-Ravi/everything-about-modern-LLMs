{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a676c075",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Embeddings are a way to turn words (or other data) into vectors of numbers so that a neural network can process them. In LLMs, they are used to convert the input text into a numerical form the model can understand and work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44bd4237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embedding parameters: 7,680,000\n",
      "\n",
      "Input token IDs: tensor([12, 45, 78])\n",
      "These might represent words like: ['the', 'cat', 'sat']\n",
      "\n",
      "Output shape: torch.Size([3, 768])\n",
      "    - 3 tokens, each represented by 768 numbers\n",
      "    - Each number in the embedding captures some learned feature about the token\n",
      "    - The embeddings are trained along with the model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create an embedding layer\n",
    "# num_embeddings=10000: vocabulary size (how many unique tokens/words we can represent)\n",
    "# embedding_dim=768: dimension of each embedding vector (how many numbers represent each word)\n",
    "embedding = nn.Embedding(num_embeddings=10000, embedding_dim=768)\n",
    "\n",
    "print(f\"Total embedding parameters: {embedding.num_embeddings * embedding.embedding_dim:,}\")\n",
    "\n",
    "# Example token indices - these represent positions in our vocabulary\n",
    "# Think of them as \"word IDs\" - each number corresponds to a specific word\n",
    "token_ids = torch.tensor([12, 45, 78])\n",
    "print(f\"\\nInput token IDs: {token_ids}\")\n",
    "print(f\"These might represent words like: ['the', 'cat', 'sat']\")\n",
    "\n",
    "# Convert token IDs to dense vectors (embeddings)\n",
    "# Each token ID gets mapped to a 768-dimensional vector of learned features\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(f\"\\nOutput shape: {embedded.shape}\")\n",
    "print(f\"    - 3 tokens, each represented by 768 numbers\")\n",
    "print(f\"    - Each number in the embedding captures some learned feature about the token\")\n",
    "print(f\"    - The embeddings are trained along with the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88f833",
   "metadata": {},
   "source": [
    "# Positional Embeddings\n",
    "Since LLMs process text in sequences, they need to know the order of the words. Positional embeddings are added to the word embeddings to give the model information about the position of each word in the sequence. This helps the model understand the context and relationships between words based on their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embedding parameters: 393,216\n",
      "\n",
      "Position IDs: tensor([0, 1, 2])\n",
      "These represent positions: [0th, 1st, 2nd] in the sequence\n",
      "\n",
      "Positional embedding shape: torch.Size([3, 768])\n",
      "    - Same shape as word embeddings: 3 tokens × 768 dimensions\n",
      "\n",
      "Final combined embeddings shape: torch.Size([3, 768])\n",
      "    - Word meaning + position information for each token\n",
      "    - Ready to be processed by transformer layers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "max_len = 512  # Context length of 512 tokens\n",
    "# This determines how much text the model can \"see\" at once\n",
    "\n",
    "# from above\n",
    "token_ids = torch.tensor([12, 45, 78])\n",
    "embedding = nn.Embedding(num_embeddings=10000, embedding_dim=768)\n",
    "pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=768)\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(f\"Positional embedding parameters: {pos_embedding.num_embeddings * pos_embedding.embedding_dim:,}\")\n",
    "\n",
    "# Create position indices for our sequence\n",
    "# Position 0, 1, 2 for our 3 tokens\n",
    "sequence_length = len(token_ids)\n",
    "position_ids = torch.arange(sequence_length)\n",
    "print(f\"\\nPosition IDs: {position_ids}\")\n",
    "print(f\"These represent positions: [0th, 1st, 2nd] in the sequence\")\n",
    "\n",
    "# Get positional embeddings\n",
    "pos_embedded = pos_embedding(position_ids)\n",
    "\n",
    "print(f\"\\nPositional embedding shape: {pos_embedded.shape}\")\n",
    "print(f\"    - Same shape as word embeddings: 3 tokens × 768 dimensions\")\n",
    "\n",
    "# Combine word embeddings with positional embeddings\n",
    "# This gives the model both semantic (what the word means) and positional (where it is) information\n",
    "final_embeddings = embedded + pos_embedded\n",
    "\n",
    "print(f\"\\nFinal combined embeddings shape: {final_embeddings.shape}\")\n",
    "print(f\"    - Word meaning + position information for each token\")\n",
    "print(f\"    - Ready to be processed by transformer layers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
