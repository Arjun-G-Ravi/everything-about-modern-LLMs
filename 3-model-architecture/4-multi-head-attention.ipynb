{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b937e68",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "Multi-Head Attention is an extension of the attention mechanism used in transformers. Instead of calculating attention once, it does it multiple times in parallel, with each version called a “head.” Each head learns to focus on different parts or aspects of the input.\n",
    "\n",
    "Multi-head attention means running attention multiple times in parallel, each time with different “views” of the input, then combining the results. This makes transformers much more powerful at modeling complex relationships in language.\n",
    "\n",
    "\n",
    "\n",
    "- The input is projected into several smaller spaces (one for each head).\n",
    "- Each head performs its own self-attention calculation independently.\n",
    "- This means each head can capture different relationships or features in the data.\n",
    "- The outputs from all heads are concatenated (joined together) and then transformed one more time with a linear layer.\n",
    "- Multiple heads allow the model to learn different kinds of relationships at the same time, improving its understanding of the context.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db687072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random input tensor x created with shape: torch.Size([10, 32, 512])\n",
      "MultiHeadAttention module initialized with 8 heads.\n",
      "Input tensor shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention output shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention weights shape: torch.Size([32, 10, 10]) (batch, sequence_len, sequence_len)\n",
      "Final output tensor shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head Attention module with clear explanations\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # Initialize PyTorch's built-in MultiheadAttention layer\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (sequence_len, batch, embedding_dim)\n",
    "        print(f\"Input tensor shape: {x.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        # Self-attention: query, key, value are all x\n",
    "        attn_output, attn_weights = self.multi_head_attention(x, x, x)\n",
    "        print(f\"Attention output shape: {attn_output.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        print(f\"Attention weights shape: {attn_weights.shape} (batch, sequence_len, sequence_len)\")\n",
    "        return attn_output\n",
    "\n",
    "# Example usage\n",
    "x = torch.rand(10, 32, 512)  # 10 tokens, batch size 32, embedding dim 512\n",
    "print(\"Random input tensor x created with shape:\", x.shape)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "print(\"MultiHeadAttention module initialized with 8 heads.\")\n",
    "output = mha(x)\n",
    "print(\"Final output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4c8b7",
   "metadata": {},
   "source": [
    "Same thing, directly implemented without nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e58f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random input tensor x created with shape: torch.Size([10, 32, 512])\n",
      "MultiHeadAttention module initialized with 8 heads.\n",
      "Input tensor shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "torch.Size([10, 32, 512])\n",
      "torch.Size([8, 10, 32, 64])\n",
      "Attention output shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention weights shape: torch.Size([8, 10, 32, 32]) (batch, num_heads, sequence_len, sequence_len)\n",
      "Final output tensor shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head Attention implemented from scratch\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim # number of numbers needed to represent one token as embeddings\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear layers for Q, K, V projections for all heads at once\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (sequence_len, batch, embedding_dim)\n",
    "        seq_len, batch_size, embed_dim = x.shape\n",
    "        print(f\"Input tensor shape: {x.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        \n",
    "        # Generate Q, K, V for all heads\n",
    "        Q = self.W_q(x)  # (seq_len, batch, embed_dim)\n",
    "        K = self.W_k(x)  # (seq_len, batch, embed_dim)\n",
    "        V = self.W_v(x)  # (seq_len, batch, embed_dim)\n",
    "        print(Q.shape)\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        # (seq_len, batch, embed_dim) -> (seq_len, batch, num_heads, head_dim) -> (batch, num_heads, seq_len, head_dim)\n",
    "        # Here, each of the heads get parts of Q, K and V. So embedding_dim is divided and given to each of the head\n",
    "        Q = Q.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        K = K.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        V = V.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        print(Q.shape)        \n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attended = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads: (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, embed_dim)\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(attended)\n",
    "        \n",
    "        # Transpose back to original format: (batch, seq_len, embed_dim) -> (seq_len, batch, embed_dim)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        print(f\"Attention output shape: {output.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        print(f\"Attention weights shape: {attn_weights.shape} (batch, num_heads, sequence_len, sequence_len)\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "x = torch.rand(10, 32, 512)  # 10 tokens, batch size 32, embedding dim 512\n",
    "print(\"Random input tensor x created with shape:\", x.shape)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "print(\"MultiHeadAttention module initialized with 8 heads.\")\n",
    "output = mha(x)\n",
    "print(\"Final output tensor shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
