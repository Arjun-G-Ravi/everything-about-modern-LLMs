{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9f49e8",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "Attention in LLMs is a mechanism that allows the model to focus on different parts of the input sequence when processing each token. In the context of transformers, it computes a weighted average of all tokens for each token, letting the model \"attend\" to relevant parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9233d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d = 4 # dimension\n",
    "\n",
    "x = torch.rand(10, d)  # (tokens, embedding_dim) # 10 tokens - input sequence\n",
    "print('Sample tokens:\\n', x)\n",
    "\n",
    "# Linear projections for queries, keys, values\n",
    "# These weight matrices learn to transform input embeddings into Q, K, V representations\n",
    "W_q = torch.rand(d, d)  # Query projection - \"what am I looking for?\"\n",
    "W_k = torch.rand(d, d)  # Key projection - \"what do I contain?\"\n",
    "W_v = torch.rand(d, d)  # Value projection - \"what information do I actually provide?\"\n",
    "\n",
    "Q = x @ W_q   # Queries: each token asks \"what should I attend to?\"\n",
    "K = x @ W_k   # Keys: each token says \"here's what I represent\"\n",
    "V = x @ W_v   # Values: each token says \"here's the info I contribute\"\n",
    "\n",
    "# Compute attention scores - how much should each token attend to every other token?\n",
    "scores = Q @ K.T / (d ** 0.5)  # (10, 10), scaled dot product attention\n",
    "# Apply softmax to get attention weights - normalize scores into probabilities\n",
    "\n",
    "weights = F.softmax(scores, dim=-1)  # (10, 10)\n",
    "# Each row sums to 1, representing attention distribution for that token\n",
    "\n",
    "# Weighted sum of values - final attended representation\n",
    "attended = weights @ V  # (10, d)\n",
    "# For each token: mix all value vectors weighted by attention scores\n",
    "# This is the core of self-attention: tokens can look at and incorporate \n",
    "# information from all other tokens in the sequence\n",
    "\n",
    "print(\"\\nAttention output:\\n\", attended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c028e",
   "metadata": {},
   "source": [
    "# Cross Attention\n",
    "Cross-attention is used in models like the Transformer decoder, where the model attends to a different sequence (e.g., encoder outputs) rather than the same sequence. This allows the decoder to focus on relevant parts of the input sequence while generating output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817beee",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "Multi-Head Attention is an extension of the attention mechanism used in transformers. Instead of calculating attention once, it does it multiple times in parallel, with each version called a “head.” Each head learns to focus on different parts or aspects of the input.\n",
    "\n",
    "Multi-head attention means running attention multiple times in parallel, each time with different “views” of the input, then combining the results. This makes transformers much more powerful at modeling complex relationships in language.\n",
    "\n",
    "\n",
    "\n",
    "- The input is projected into several smaller spaces (one for each head).\n",
    "- Each head performs its own self-attention calculation independently.\n",
    "- This means each head can capture different relationships or features in the data.\n",
    "- The outputs from all heads are concatenated (joined together) and then transformed one more time with a linear layer.\n",
    "- Multiple heads allow the model to learn different kinds of relationships at the same time, improving its understanding of the context.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef628ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random input tensor x created with shape: torch.Size([10, 32, 512])\n",
      "MultiHeadAttention module initialized with 8 heads.\n",
      "Input tensor shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention output shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention weights shape: torch.Size([32, 10, 10]) (batch, sequence_len, sequence_len)\n",
      "Final output tensor shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head Attention module with clear explanations\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # Initialize PyTorch's built-in MultiheadAttention layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (sequence_len, batch, embedding_dim)\n",
    "        print(f\"Input tensor shape: {x.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        # Self-attention: query, key, value are all x\n",
    "        attn_output, attn_weights = self.attention(x, x, x)\n",
    "        print(f\"Attention output shape: {attn_output.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        print(f\"Attention weights shape: {attn_weights.shape} (batch, sequence_len, sequence_len)\")\n",
    "        return attn_output\n",
    "\n",
    "# Example usage\n",
    "x = torch.rand(10, 32, 512)  # 10 tokens, batch size 32, embedding dim 512\n",
    "print(\"Random input tensor x created with shape:\", x.shape)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "print(\"MultiHeadAttention module initialized with 8 heads.\")\n",
    "output = mha(x)\n",
    "print(\"Final output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93f493",
   "metadata": {},
   "source": [
    "Same thing, directly implemented without nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fc072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random input tensor x created with shape: torch.Size([10, 32, 512])\n",
      "MultiHeadAttention module initialized with 8 heads.\n",
      "Input tensor shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "torch.Size([10, 32, 512])\n",
      "torch.Size([8, 10, 32, 64])\n",
      "Attention output shape: torch.Size([10, 32, 512]) (sequence_len, batch, embedding_dim)\n",
      "Attention weights shape: torch.Size([8, 10, 32, 32]) (batch, num_heads, sequence_len, sequence_len)\n",
      "Final output tensor shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head Attention implemented from scratch\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim # number of numbers needed to represent one token as embeddings\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear layers for Q, K, V projections for all heads at once\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (sequence_len, batch, embedding_dim)\n",
    "        seq_len, batch_size, embed_dim = x.shape\n",
    "        print(f\"Input tensor shape: {x.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        \n",
    "        # Generate Q, K, V for all heads\n",
    "        Q = self.W_q(x)  # (seq_len, batch, embed_dim)\n",
    "        K = self.W_k(x)  # (seq_len, batch, embed_dim)\n",
    "        V = self.W_v(x)  # (seq_len, batch, embed_dim)\n",
    "        print(Q.shape)\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        # (seq_len, batch, embed_dim) -> (seq_len, batch, num_heads, head_dim) -> (batch, num_heads, seq_len, head_dim)\n",
    "        # Here, each of the heads get parts of Q, K and V. So embedding_dim is divided and given to each of the head\n",
    "        Q = Q.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        K = K.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        V = V.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 2).transpose(1, 2)\n",
    "        print(Q.shape)        \n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attended = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads: (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, embed_dim)\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(attended)\n",
    "        \n",
    "        # Transpose back to original format: (batch, seq_len, embed_dim) -> (seq_len, batch, embed_dim)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        print(f\"Attention output shape: {output.shape} (sequence_len, batch, embedding_dim)\")\n",
    "        print(f\"Attention weights shape: {attn_weights.shape} (batch, num_heads, sequence_len, sequence_len)\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "x = torch.rand(10, 32, 512)  # 10 tokens, batch size 32, embedding dim 512\n",
    "print(\"Random input tensor x created with shape:\", x.shape)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "print(\"MultiHeadAttention module initialized with 8 heads.\")\n",
    "output = mha(x)\n",
    "print(\"Final output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c326ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbcf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
